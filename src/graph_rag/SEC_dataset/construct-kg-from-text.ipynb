{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2979585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Common data processing\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Langchain\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ovgcc963pah",
   "metadata": {},
   "source": [
    "# Building a Knowledge Graph from SEC Filings for Graph RAG\n",
    "\n",
    "This tutorial demonstrates how to transform unstructured text documents (SEC 10-K filings) into a queryable knowledge graph with vector search capabilities. We'll build a complete Graph RAG pipeline that combines semantic search with graph structure.\n",
    "\n",
    "## What is Graph RAG?\n",
    "\n",
    "**Graph RAG** = Traditional RAG (Retrieval Augmented Generation) + Knowledge Graphs\n",
    "\n",
    "### Traditional RAG Limitations:\n",
    "- Treats documents as isolated chunks\n",
    "- No understanding of relationships between entities\n",
    "- Can't answer multi-hop questions like \"Which firms connected to X have invested over $Y?\"\n",
    "\n",
    "### Graph RAG Solution:\n",
    "1. **Chunk documents** into manageable pieces\n",
    "2. **Create graph nodes** for each chunk with metadata\n",
    "3. **Generate embeddings** for semantic search\n",
    "4. **Extract entities and relationships** (future: add company, person, investment nodes)\n",
    "5. **Query using both** vector similarity AND graph traversal\n",
    "\n",
    "## Real-World Use Case: Financial Analysis\n",
    "\n",
    "Imagine asking: *\"Which companies connected to Palo Alto Networks have disclosed cybersecurity risks and invested over $50M in R&D?\"*\n",
    "\n",
    "This requires:\n",
    "- Semantic search for \"cybersecurity risks\" (vector similarity)\n",
    "- Entity extraction for companies and financial figures\n",
    "- Graph traversal for \"connected to\" relationships\n",
    "- Filtering by investment amounts\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "We're using **NetApp's 10-K Form** (SEC filing 0000950170-23-027948):\n",
    "- **Item 1**: Business Overview\n",
    "- **Item 1A**: Risk Factors  \n",
    "- **Item 7**: Management's Discussion and Analysis\n",
    "- **Item 7A**: Market Risk Disclosures\n",
    "\n",
    "After processing, we'll have:\n",
    "- **257 text chunks** (nodes in our graph)\n",
    "- **Vector embeddings** for each chunk (1536 dimensions)\n",
    "- **Metadata**: company name, CIK, CUSIP, form section, source URL\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you'll understand:\n",
    "\n",
    "1. **Document Chunking**: How to split long documents while preserving context\n",
    "2. **Graph Construction**: Creating nodes with Cypher MERGE statements\n",
    "3. **Vector Indexing**: Setting up Neo4j vector indexes for similarity search\n",
    "4. **Embedding Generation**: Using OpenAI embeddings within Neo4j\n",
    "5. **Retrieval Augmented Generation**: Building a Q&A system with LangChain\n",
    "6. **Hallucination Control**: Testing how the system handles out-of-scope questions\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64b1bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from environment\n",
    "load_dotenv()\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_DATABASE = os.getenv('NEO4J_DATABASE') or 'neo4j'\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "OPENAI_ENDPOINT = os.getenv('OPENAI_BASE_URL') + '/embeddings'\n",
    "\n",
    "# Global constants\n",
    "VECTOR_INDEX_NAME = 'form_10k_chunks'\n",
    "VECTOR_NODE_LABEL = 'Chunk'\n",
    "VECTOR_SOURCE_PROPERTY = 'text'\n",
    "VECTOR_EMBEDDING_PROPERTY = 'textEmbedding'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l125pmubq2",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, we'll load environment variables and set up our connections to Neo4j and OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ef4829f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded: sk-proj-cZujNxtOyhQE...\n",
      "Endpoint: https://api.openai.com/v1/embeddings\n"
     ]
    }
   ],
   "source": [
    "print(f\"API Key loaded: {OPENAI_API_KEY[:20]}...\" if OPENAI_API_KEY else \"API Key is None!\")\n",
    "print(f\"Endpoint: {OPENAI_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e6a9d5",
   "metadata": {},
   "source": [
    "## Step 1: Load and Explore the SEC Filing Data\n",
    "\n",
    "SEC **Form 10-K** is an annual report required by the U.S. Securities and Exchange Commission (SEC). It provides a comprehensive summary of a company's financial performance.\n",
    "\n",
    "### Key Sections We're Processing:\n",
    "- **Item 1**: Description of business operations\n",
    "- **Item 1A**: Risk factors\n",
    "- **Item 7**: Management's discussion and analysis (MD&A)\n",
    "- **Item 7A**: Quantitative and qualitative disclosures about market risk\n",
    "\n",
    "You can search and download these filings from the SEC's [EDGAR database](https://www.sec.gov/edgar/search/).\n",
    "\n",
    "Our data file is a JSON containing the NetApp 10-K filing with pre-extracted sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d11261ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of the file: <class 'dict'>\n",
      "item1 <class 'str'>\n",
      "item1a <class 'str'>\n",
      "item7 <class 'str'>\n",
      "item7a <class 'str'>\n",
      "cik <class 'str'>\n",
      "cusip6 <class 'str'>\n",
      "cusip <class 'list'>\n",
      "names <class 'list'>\n",
      "source <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "file_name = \"0000950170-23-027948.json\"\n",
    "file = json.load(open(file_name))\n",
    "print(f\"Type of the file: {type(file)}\")\n",
    "\n",
    "for k, v in file.items():\n",
    "    print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e48b874b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>Item 1.  \\nBusiness\\n\\n\\nOverview\\n\\n\\nNetApp, Inc. (NetApp, we, us or the Company) is a global cloud-led, data-centric software company. We were incorporated in 1992 and are headquartered in San Jose, California. Building on more than three decades of innovation, we give customers the freedom to manage applications and data across hybrid multicloud environments. Our portfolio of cloud services, and storage infrastructure, powered by intelligent data management software, enables applications to run faster, more reliably, and more securely, all at a lower cost.\\n\\n\\nOur opportunity is defined by the durable megatrends of data-driven digital and cloud transformations. NetApp helps organizations meet the complexities created by rapid data and cloud growth, multi-cloud management, and the adoption of next-generation technologies, such as AI, Kubernetes, and modern databases. Our modern approach to hybrid, multicloud infrastructure and data management, which we term ‘evolved cloud’, provides custome'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item1_text = file['item1']\n",
    "item1_text[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042805bd",
   "metadata": {},
   "source": [
    "## Step 2: Document Chunking Strategy\n",
    "\n",
    "### Why Chunking Matters\n",
    "\n",
    "**The Challenge**: LLMs have context window limits, and embeddings work best on focused text segments.\n",
    "\n",
    "**The Goal**: Split documents into chunks that:\n",
    "1. Fit within embedding model limits (we use OpenAI's ada-002: max 8,191 tokens)\n",
    "2. Preserve semantic coherence (don't split mid-sentence or mid-paragraph)\n",
    "3. Maintain some context overlap (help with boundary cases)\n",
    "\n",
    "### Our Chunking Parameters:\n",
    "- **chunk_size = 2000 characters**: ~400-500 tokens (safe for embeddings)\n",
    "- **chunk_overlap = 200 characters**: Overlapping context prevents information loss at boundaries\n",
    "- **RecursiveCharacterTextSplitter**: LangChain's splitter that tries to split on paragraph breaks first, then sentences\n",
    "\n",
    "### Visual Example:\n",
    "```\n",
    "Document: \"AAAAA BBBBB CCCCC DDDDD EEEEE\"\n",
    "\n",
    "Chunk 1: \"AAAAA BBBBB CCCCC\"\n",
    "                        ↓ (200 char overlap)\n",
    "Chunk 2:         \"CCCCC DDDDD EEEEE\"\n",
    "```\n",
    "\n",
    "This overlap ensures queries about \"CCCCC\" match both chunks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe73350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccd6e770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0000950170-23-027948'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name[:file_name.rindex('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3899062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_form10k_data_into_chunks(file_name):\n",
    "    chunks_with_metadata = []\n",
    "    file = json.load(open(file_name))\n",
    "    for item in ['item1', 'item1a', 'item7', 'item7a']:\n",
    "        print(f'Processing {item} from {file_name}')\n",
    "        item_text = file[item]\n",
    "        item_text_chunks = text_splitter.split_text(item_text)\n",
    "        chunk_seq_id = 0\n",
    "\n",
    "        for chunk in item_text_chunks:\n",
    "            form_id = file_name[:file_name.rindex('.')]\n",
    "            chunks_with_metadata.append({\n",
    "                'text': chunk,\n",
    "                'form10kItem': item,\n",
    "                'chunkSeqId': chunk_seq_id,\n",
    "                'formId': f'{form_id}',\n",
    "                'chunkId': f'{form_id}-{item}-chunk{chunk_seq_id:04d}',\n",
    "                # metadata\n",
    "                'names': file['names'],\n",
    "                'cik': file['cik'],\n",
    "                'cusip6': file['cusip6'],\n",
    "                'source': file['source']\n",
    "            })\n",
    "            chunk_seq_id += 1\n",
    "        print(f'Split into {chunk_seq_id} chunks')\n",
    "    return chunks_with_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a276fdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing item1 from 0000950170-23-027948.json\n",
      "Split into 254 chunks\n",
      "Processing item1a from 0000950170-23-027948.json\n",
      "Split into 1 chunks\n",
      "Processing item7 from 0000950170-23-027948.json\n",
      "Split into 1 chunks\n",
      "Processing item7a from 0000950170-23-027948.json\n",
      "Split into 1 chunks\n"
     ]
    }
   ],
   "source": [
    "file_chunks = split_form10k_data_into_chunks(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9b520b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '>Item 1.  \\nBusiness\\n\\n\\nOverview\\n\\n\\nNetApp, Inc. (NetApp, we, us or the Company) is a global cloud-led, data-centric software company. We were incorporated in 1992 and are headquartered in San Jose, California. Building on more than three decades of innovation, we give customers the freedom to manage applications and data across hybrid multicloud environments. Our portfolio of cloud services, and storage infrastructure, powered by intelligent data management software, enables applications to run faster, more reliably, and more securely, all at a lower cost.\\n\\n\\nOur opportunity is defined by the durable megatrends of data-driven digital and cloud transformations. NetApp helps organizations meet the complexities created by rapid data and cloud growth, multi-cloud management, and the adoption of next-generation technologies, such as AI, Kubernetes, and modern databases. Our modern approach to hybrid, multicloud infrastructure and data management, which we term ‘evolved cloud’, provides customers the ability to leverage data across their entire estate with simplicity, security, and sustainability which increases our relevance and value to our customers.\\n\\n\\nIn an evolved cloud state, the cloud is fully integrated into an organization’s architecture and operations. Data centers and clouds are seamlessly united and hybrid multicloud operations are simplified, with consistency and observability across environments. The key benefits NetApp brings to an organization’s hybrid multicloud environment are:\\n\\n\\n•\\nOperational simplicity: NetApp’s use of open source, open architectures and APIs, microservices, and common capabilities and data services facilitate the creation of applications that can run anywhere.\\n\\n\\n•\\nFlexibility and consistency: NetApp makes moving data and applications between environments seamless through a common storage foundation across on-premises and multicloud environments.',\n",
       " 'form10kItem': 'item1',\n",
       " 'chunkSeqId': 0,\n",
       " 'formId': '0000950170-23-027948',\n",
       " 'chunkId': '0000950170-23-027948-item1-chunk0000',\n",
       " 'names': ['Netapp Inc', 'NETAPP INC'],\n",
       " 'cik': '1002047',\n",
       " 'cusip6': '64110D',\n",
       " 'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744b8ee1",
   "metadata": {},
   "source": [
    "### Understanding Our Chunk Metadata\n",
    "\n",
    "Each chunk contains:\n",
    "- **text**: The actual text content\n",
    "- **chunkId**: Unique identifier (e.g., `0000950170-23-027948-item1-chunk0000`)\n",
    "- **formId**: SEC filing ID\n",
    "- **form10kItem**: Which section (item1, item1a, item7, item7a)\n",
    "- **chunkSeqId**: Sequential order within that section (0, 1, 2, ...)\n",
    "- **names**: Company names (e.g., ['Netapp Inc', 'NETAPP INC'])\n",
    "- **cik**: Central Index Key (SEC's unique company identifier)\n",
    "- **cusip6**: First 6 characters of CUSIP (financial security identifier)\n",
    "- **source**: URL to the original SEC filing\n",
    "\n",
    "This rich metadata enables:\n",
    "- **Filtering**: \"Only show me risk factors\" (filter by form10kItem = 'item1a')\n",
    "- **Provenance**: Track answers back to source documents\n",
    "- **Entity resolution**: Link chunks to the same company via CIK\n",
    "- **Sequential reading**: Use chunkSeqId to read in order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ceff5a",
   "metadata": {},
   "source": [
    "## Step 3: Create Graph Nodes from Text Chunks\n",
    "\n",
    "Now we'll transform our Python dictionaries into graph nodes using Cypher, Neo4j's query language.\n",
    "\n",
    "### Understanding the MERGE Pattern\n",
    "\n",
    "**MERGE** is like \"INSERT or UPDATE\" in SQL:\n",
    "- If a node with `chunkId` exists → do nothing\n",
    "- If it doesn't exist → create it and set all properties\n",
    "\n",
    "### Why MERGE instead of CREATE?\n",
    "- **Idempotent**: Safe to run multiple times without creating duplicates\n",
    "- **Upsert logic**: Can update existing nodes (though we just create here)\n",
    "\n",
    "### The Cypher Query Breakdown:\n",
    "```cypher\n",
    "MERGE (mergedChunk:Chunk {chunkId: $chunkParam.chunkId})\n",
    "  -- Find or create a Chunk node with this chunkId\n",
    "  \n",
    "ON CREATE SET\n",
    "  -- Only runs if the node was newly created\n",
    "  mergedChunk.names = $chunkParam.names,\n",
    "  mergedChunk.formId = $chunkParam.formId,\n",
    "  -- ... set all other properties\n",
    "  \n",
    "RETURN mergedChunk\n",
    "  -- Return the node (for verification)\n",
    "```\n",
    "\n",
    "### Parameters with `$chunkParam`:\n",
    "We pass the entire chunk dictionary as a parameter. Neo4j automatically maps nested properties:\n",
    "- `$chunkParam.chunkId` → `file_chunks[0]['chunkId']`\n",
    "- `$chunkParam.text` → `file_chunks[0]['text']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92366d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_node_query = \"\"\"\n",
    "merge(mergedChunk:Chunk {chunkId: $chunkParam.chunkId})\n",
    "on create set\n",
    "    mergedChunk.names = $chunkParam.names,\n",
    "    mergedChunk.formId = $chunkParam.formId, \n",
    "    mergedChunk.cik = $chunkParam.cik, \n",
    "    mergedChunk.cusip6 = $chunkParam.cusip6, \n",
    "    mergedChunk.source = $chunkParam.source, \n",
    "    mergedChunk.f10kItem = $chunkParam.form10kItem, \n",
    "    mergedChunk.chunkSeqId = $chunkParam.chunkSeqId, \n",
    "    mergedChunk.text = $chunkParam.text\n",
    "return mergedChunk\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b01dd32",
   "metadata": {},
   "source": [
    "Connection to graph instance using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "911f4e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = Neo4jGraph(\n",
    "    url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc43ce6e",
   "metadata": {},
   "source": [
    "Remove the movies dataset, if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0eda79a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie dataset removed\n"
     ]
    }
   ],
   "source": [
    "kg.query(\"\"\"\n",
    "      MATCH (n:Movie) DETACH DELETE n\n",
    "  \"\"\")\n",
    "kg.query(\"\"\"\n",
    "    MATCH (n:Person) DETACH DELETE n\n",
    "\"\"\")\n",
    "print(\"Movie dataset removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c7ea8f",
   "metadata": {},
   "source": [
    "Create a single chunk node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98c80f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'mergedChunk': {'formId': '0000950170-23-027948',\n",
       "   'f10kItem': 'item1',\n",
       "   'names': ['Netapp Inc', 'NETAPP INC'],\n",
       "   'cik': '1002047',\n",
       "   'cusip6': '64110D',\n",
       "   'source': 'https://www.sec.gov/Archives/edgar/data/1002047/000095017023027948/0000950170-23-027948-index.htm',\n",
       "   'text': '>Item 1.  \\nBusiness\\n\\n\\nOverview\\n\\n\\nNetApp, Inc. (NetApp, we, us or the Company) is a global cloud-led, data-centric software company. We were incorporated in 1992 and are headquartered in San Jose, California. Building on more than three decades of innovation, we give customers the freedom to manage applications and data across hybrid multicloud environments. Our portfolio of cloud services, and storage infrastructure, powered by intelligent data management software, enables applications to run faster, more reliably, and more securely, all at a lower cost.\\n\\n\\nOur opportunity is defined by the durable megatrends of data-driven digital and cloud transformations. NetApp helps organizations meet the complexities created by rapid data and cloud growth, multi-cloud management, and the adoption of next-generation technologies, such as AI, Kubernetes, and modern databases. Our modern approach to hybrid, multicloud infrastructure and data management, which we term ‘evolved cloud’, provides customers the ability to leverage data across their entire estate with simplicity, security, and sustainability which increases our relevance and value to our customers.\\n\\n\\nIn an evolved cloud state, the cloud is fully integrated into an organization’s architecture and operations. Data centers and clouds are seamlessly united and hybrid multicloud operations are simplified, with consistency and observability across environments. The key benefits NetApp brings to an organization’s hybrid multicloud environment are:\\n\\n\\n•\\nOperational simplicity: NetApp’s use of open source, open architectures and APIs, microservices, and common capabilities and data services facilitate the creation of applications that can run anywhere.\\n\\n\\n•\\nFlexibility and consistency: NetApp makes moving data and applications between environments seamless through a common storage foundation across on-premises and multicloud environments.',\n",
       "   'chunkId': '0000950170-23-027948-item1-chunk0000',\n",
       "   'chunkSeqId': 0}}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query(chunk_node_query, params={'chunkParam': file_chunks[0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac803cf0",
   "metadata": {},
   "source": [
    "## Step 4: Create Constraints for Data Integrity\n",
    "\n",
    "### What are Graph Constraints?\n",
    "\n",
    "Like SQL constraints, Neo4j constraints enforce data rules:\n",
    "\n",
    "1. **Uniqueness**: No two nodes can have the same property value\n",
    "2. **Existence**: A property must exist on all nodes of a label\n",
    "3. **Property type**: Enforce data types\n",
    "\n",
    "### Why `UNIQUE` on chunkId?\n",
    "\n",
    "```cypher\n",
    "CREATE CONSTRAINT unique_chunk IF NOT EXISTS\n",
    "  FOR (c:Chunk) REQUIRE c.chunkId IS UNIQUE\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- **Prevents duplicates**: Can't accidentally create two chunks with same ID\n",
    "- **Creates an index**: Automatic index on `chunkId` for fast lookups\n",
    "- **MERGE performance**: Makes `MERGE` operations much faster (uses index)\n",
    "\n",
    "**Real-world analogy**: Like a primary key in SQL databases.\n",
    "\n",
    "### Checking Existing Indexes\n",
    "\n",
    "The `SHOW INDEXES` command reveals:\n",
    "- **VECTOR indexes**: For similarity search (we'll create one next)\n",
    "- **RANGE indexes**: For property lookups (created by constraints)\n",
    "- **LOOKUP indexes**: Internal Neo4j indexes for labels/relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d09744fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query(\"\"\"\n",
    "    create constraint unique_chunk if not exists\n",
    "         for (c:Chunk) require c.chunkId is unique\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01072abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 4,\n",
       "  'name': 'form_10k_chunks',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'VECTOR',\n",
       "  'entityType': 'NODE',\n",
       "  'labelsOrTypes': ['Chunk'],\n",
       "  'properties': ['textEmbedding'],\n",
       "  'indexProvider': 'vector-3.0',\n",
       "  'owningConstraint': None,\n",
       "  'lastRead': neo4j.time.DateTime(2025, 12, 10, 1, 48, 35, 932000000, tzinfo=<UTC>),\n",
       "  'readCount': 9},\n",
       " {'id': 1,\n",
       "  'name': 'index_1b9dcc97',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'LOOKUP',\n",
       "  'entityType': 'RELATIONSHIP',\n",
       "  'labelsOrTypes': None,\n",
       "  'properties': None,\n",
       "  'indexProvider': 'token-lookup-1.0',\n",
       "  'owningConstraint': None,\n",
       "  'lastRead': neo4j.time.DateTime(2025, 12, 5, 7, 13, 47, 53000000, tzinfo=<UTC>),\n",
       "  'readCount': 20},\n",
       " {'id': 0,\n",
       "  'name': 'index_460996c0',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'LOOKUP',\n",
       "  'entityType': 'NODE',\n",
       "  'labelsOrTypes': None,\n",
       "  'properties': None,\n",
       "  'indexProvider': 'token-lookup-1.0',\n",
       "  'owningConstraint': None,\n",
       "  'lastRead': neo4j.time.DateTime(2025, 12, 11, 16, 40, 29, 902000000, tzinfo=<UTC>),\n",
       "  'readCount': 374},\n",
       " {'id': 2,\n",
       "  'name': 'unique_chunk',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'RANGE',\n",
       "  'entityType': 'NODE',\n",
       "  'labelsOrTypes': ['Chunk'],\n",
       "  'properties': ['chunkId'],\n",
       "  'indexProvider': 'range-1.0',\n",
       "  'owningConstraint': 'unique_chunk',\n",
       "  'lastRead': neo4j.time.DateTime(2025, 12, 11, 16, 40, 34, 943000000, tzinfo=<UTC>),\n",
       "  'readCount': 1034}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query('show indexes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089c7dbe",
   "metadata": {},
   "source": [
    "- Loop through and create nodes for all chunks\n",
    "- Should create 257 chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9cde6201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0001\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0002\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0003\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0004\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0005\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0006\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0007\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0008\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0009\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0010\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0011\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0012\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0013\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0014\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0015\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0016\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0017\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0018\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0019\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0020\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0021\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0022\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0023\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0024\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0025\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0026\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0027\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0028\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0029\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0030\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0031\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0032\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0033\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0034\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0035\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0036\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0037\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0038\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0039\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0040\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0041\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0042\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0043\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0044\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0045\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0046\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0047\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0048\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0049\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0050\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0051\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0052\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0053\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0054\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0055\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0056\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0057\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0058\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0059\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0060\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0061\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0062\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0063\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0064\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0065\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0066\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0067\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0068\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0069\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0070\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0071\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0072\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0073\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0074\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0075\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0076\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0077\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0078\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0079\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0080\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0081\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0082\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0083\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0084\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0085\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0086\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0087\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0088\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0089\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0090\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0091\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0092\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0093\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0094\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0095\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0096\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0097\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0098\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0099\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0100\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0101\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0102\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0103\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0104\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0105\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0106\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0107\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0108\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0109\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0110\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0111\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0112\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0113\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0114\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0115\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0116\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0117\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0118\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0119\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0120\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0121\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0122\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0123\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0124\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0125\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0126\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0127\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0128\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0129\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0130\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0131\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0132\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0133\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0134\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0135\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0136\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0137\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0138\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0139\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0140\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0141\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0142\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0143\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0144\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0145\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0146\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0147\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0148\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0149\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0150\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0151\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0152\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0153\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0154\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0155\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0156\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0157\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0158\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0159\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0160\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0161\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0162\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0163\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0164\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0165\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0166\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0167\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0168\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0169\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0170\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0171\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0172\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0173\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0174\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0175\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0176\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0177\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0178\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0179\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0180\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0181\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0182\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0183\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0184\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0185\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0186\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0187\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0188\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0189\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0190\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0191\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0192\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0193\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0194\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0195\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0196\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0197\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0198\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0199\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0200\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0201\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0202\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0203\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0204\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0205\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0206\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0207\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0208\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0209\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0210\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0211\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0212\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0213\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0214\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0215\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0216\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0217\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0218\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0219\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0220\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0221\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0222\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0223\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0224\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0225\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0226\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0227\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0228\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0229\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0230\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0231\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0232\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0233\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0234\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0235\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0236\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0237\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0238\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0239\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0240\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0241\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0242\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0243\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0244\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0245\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0246\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0247\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0248\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0249\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0250\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0251\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0252\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1-chunk0253\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item1a-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item7-chunk0000\n",
      "Creating `:Chunk` node for chunk ID 0000950170-23-027948-item7a-chunk0000\n",
      "Created 257 nodes\n"
     ]
    }
   ],
   "source": [
    "node_count = 0\n",
    "for chunk in file_chunks:\n",
    "    print(f\"Creating `:Chunk` node for chunk ID {chunk['chunkId']}\")\n",
    "\n",
    "    kg.query(chunk_node_query, params={'chunkParam': chunk})\n",
    "    node_count += 1\n",
    "print(f'Created {node_count} nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6d607ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'nodeCount': 257}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query(\"\"\"match (n) \n",
    "         return count(n) as nodeCount\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b9ea8",
   "metadata": {},
   "source": [
    "## Step 5: Create a Vector Index for Semantic Search\n",
    "\n",
    "### What is a Vector Index?\n",
    "\n",
    "Traditional indexes work with exact matches or ranges:\n",
    "- \"Find chunks where formId = 'X'\" → Range index\n",
    "- \"Find chunks where name contains 'NetApp'\" → Text index\n",
    "\n",
    "**Vector indexes** enable **similarity search**:\n",
    "- \"Find chunks semantically similar to this query\"\n",
    "- Uses cosine similarity, Euclidean distance, or other metrics\n",
    "- Essential for RAG systems\n",
    "\n",
    "### Understanding the Configuration\n",
    "\n",
    "```cypher\n",
    "CREATE VECTOR INDEX `form_10k_chunks` IF NOT EXISTS\n",
    "  FOR (c:Chunk) ON (c.textEmbedding)\n",
    "  OPTIONS {\n",
    "    indexConfig: {\n",
    "      `vector.dimensions`: 1536,  -- OpenAI ada-002 embedding size\n",
    "      `vector.similarity_function`: 'cosine'  -- Cosine similarity\n",
    "    }\n",
    "  }\n",
    "```\n",
    "\n",
    "### Key Parameters:\n",
    "\n",
    "| Parameter | Value | Why? |\n",
    "|-----------|-------|------|\n",
    "| **dimensions** | 1536 | OpenAI's text-embedding-ada-002 produces 1536-dim vectors |\n",
    "| **similarity_function** | cosine | Best for text embeddings (normalizes for length) |\n",
    "\n",
    "### Similarity Functions Compared:\n",
    "\n",
    "- **Cosine**: Measures angle between vectors (0 to 1, higher = more similar)\n",
    "  - Best for: Text, where magnitude doesn't matter\n",
    "- **Euclidean**: Measures straight-line distance\n",
    "  - Best for: Spatial data, images\n",
    "- **Dot Product**: Considers both angle and magnitude\n",
    "  - Best for: When vector magnitude has meaning\n",
    "\n",
    "**For text embeddings, always use cosine similarity!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24bfe234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query(\"\"\"\n",
    "    create vector index `form_10k_chunks` if not exists\n",
    "         for (c: Chunk) on (c.textEmbedding)\n",
    "         options { indexConfig: {\n",
    "            `vector.dimensions`: 1536,\n",
    "            `vector.similarity_function`: 'cosine'\n",
    "         }}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "591189e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 4,\n",
       "  'name': 'form_10k_chunks',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'VECTOR',\n",
       "  'entityType': 'NODE',\n",
       "  'labelsOrTypes': ['Chunk'],\n",
       "  'properties': ['textEmbedding'],\n",
       "  'indexProvider': 'vector-3.0',\n",
       "  'owningConstraint': None,\n",
       "  'lastRead': neo4j.time.DateTime(2025, 12, 10, 1, 48, 35, 932000000, tzinfo=<UTC>),\n",
       "  'readCount': 9},\n",
       " {'id': 1,\n",
       "  'name': 'index_1b9dcc97',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'LOOKUP',\n",
       "  'entityType': 'RELATIONSHIP',\n",
       "  'labelsOrTypes': None,\n",
       "  'properties': None,\n",
       "  'indexProvider': 'token-lookup-1.0',\n",
       "  'owningConstraint': None,\n",
       "  'lastRead': neo4j.time.DateTime(2025, 12, 5, 7, 13, 47, 53000000, tzinfo=<UTC>),\n",
       "  'readCount': 20},\n",
       " {'id': 0,\n",
       "  'name': 'index_460996c0',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'LOOKUP',\n",
       "  'entityType': 'NODE',\n",
       "  'labelsOrTypes': None,\n",
       "  'properties': None,\n",
       "  'indexProvider': 'token-lookup-1.0',\n",
       "  'owningConstraint': None,\n",
       "  'lastRead': neo4j.time.DateTime(2025, 12, 11, 16, 40, 29, 902000000, tzinfo=<UTC>),\n",
       "  'readCount': 374},\n",
       " {'id': 2,\n",
       "  'name': 'unique_chunk',\n",
       "  'state': 'ONLINE',\n",
       "  'populationPercent': 100.0,\n",
       "  'type': 'RANGE',\n",
       "  'entityType': 'NODE',\n",
       "  'labelsOrTypes': ['Chunk'],\n",
       "  'properties': ['chunkId'],\n",
       "  'indexProvider': 'range-1.0',\n",
       "  'owningConstraint': 'unique_chunk',\n",
       "  'lastRead': neo4j.time.DateTime(2025, 12, 11, 16, 41, 8, 496000000, tzinfo=<UTC>),\n",
       "  'readCount': 1803}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query('show indexes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868092be",
   "metadata": {},
   "source": [
    "## Step 6: Generate and Store Embeddings\n",
    "\n",
    "This is where the magic happens! We'll calculate embedding vectors for all chunks and store them directly in Neo4j.\n",
    "\n",
    "### Neo4j GenAI Plugin\n",
    "\n",
    "Neo4j 5.x includes built-in GenAI functions that call embedding APIs from within Cypher queries:\n",
    "\n",
    "```cypher\n",
    "genai.vector.encode(\n",
    "  text,              -- The text to embed\n",
    "  \"OpenAI\",          -- Provider\n",
    "  {\n",
    "    token: $openAiApiKey,\n",
    "    endpoint: $openAiEndpoint\n",
    "  }\n",
    ")\n",
    "```\n",
    "\n",
    "### The Complete Query Breakdown:\n",
    "\n",
    "```cypher\n",
    "MATCH (chunk:Chunk) WHERE chunk.textEmbedding IS NULL\n",
    "  -- Only process chunks without embeddings (idempotent)\n",
    "\n",
    "WITH chunk, genai.vector.encode(...) AS vector\n",
    "  -- Generate embedding for chunk.text\n",
    "  \n",
    "CALL db.create.setNodeVectorProperty(chunk, \"textEmbedding\", vector)\n",
    "  -- Store the vector as a property on the node\n",
    "```\n",
    "\n",
    "### Why Store Embeddings in the Graph?\n",
    "\n",
    "**Option 1**: External vector DB (Pinecone, Weaviate, etc.)\n",
    "- Pros: Specialized for vectors, fast\n",
    "- Cons: Two databases to manage, sync issues\n",
    "\n",
    "**Option 2**: Store in Neo4j (our approach)\n",
    "- Pros: Single source of truth, use graph structure + vectors together\n",
    "- Cons: Slightly slower for pure vector search\n",
    "\n",
    "**For Graph RAG, option 2 is better!** We need both graph traversal and vector search in the same query.\n",
    "\n",
    "### Cost Consideration\n",
    "\n",
    "Embedding 257 chunks with OpenAI ada-002:\n",
    "- ~500K characters total\n",
    "- Cost: ~$0.0001 per 1K tokens → ~$0.05 total\n",
    "- One-time cost (embeddings are stored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "789fa848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query(\"\"\"\n",
    "    match (chunk: Chunk) where chunk.textEmbedding is null\n",
    "    with chunk, genai.vector.encode(\n",
    "        chunk.text,\n",
    "        \"OpenAI\",\n",
    "        {\n",
    "            token: $openAiApiKey,\n",
    "            endpoint: $openAiEndpoint\n",
    "        }\n",
    "    ) as vector\n",
    "    call db.create.setNodeVectorProperty(chunk, \"textEmbedding\", vector)\n",
    "\"\"\",\n",
    "params={\"openAiApiKey\":OPENAI_API_KEY, \"openAiEndpoint\": OPENAI_ENDPOINT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ebea4f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[#CC7E]  _: <CONNECTION> error: Failed to read from defunct connection IPv4Address(('p-c43bf5af-d06d-0001.production-orch-1048.neo4j.io', 7687)) (ResolvedIPv4Address(('54.85.127.23', 7687))): OSError('No data')\n",
      "Transaction failed and will be retried in 0.9544950658835788s (Failed to read from defunct connection IPv4Address(('p-c43bf5af-d06d-0001.production-orch-1048.neo4j.io', 7687)) (ResolvedIPv4Address(('54.85.127.23', 7687))))\n",
      "[#CC8A]  _: <CONNECTION> error: Failed to read from defunct connection ResolvedIPv4Address(('44.210.31.186', 7687)) (ResolvedIPv4Address(('44.210.31.186', 7687))): OSError('No data')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'labeled': 0}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg.query(\"\"\"\n",
    "    MATCH (n)\n",
    "    WHERE n.chunkId IS NOT NULL AND NOT n:Chunk\n",
    "    SET n:Chunk\n",
    "    RETURN count(n) as labeled\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b4d0770e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties:\n",
      "Chunk {chunkId: STRING, names: LIST, formId: STRING, cik: STRING, cusip6: STRING, source: STRING, chunkSeqId: INTEGER, text: STRING, textEmbedding: LIST, f10kItem: STRING}\n",
      "Relationship properties:\n",
      "\n",
      "The relationships:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kg.refresh_schema()\n",
    "print(kg.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae146d3c",
   "metadata": {},
   "source": [
    "## Step 7: Implement Vector Similarity Search\n",
    "\n",
    "Now we can query our knowledge graph using natural language questions!\n",
    "\n",
    "### How Vector Search Works\n",
    "\n",
    "1. **Encode the question**: Convert user's query into a 1536-dim embedding vector\n",
    "2. **Calculate similarity**: Compare question embedding to all chunk embeddings\n",
    "3. **Rank by score**: Return top-k most similar chunks (we use k=10)\n",
    "4. **Return results**: Get the text and similarity scores\n",
    "\n",
    "### The Vector Search Query\n",
    "\n",
    "```cypher\n",
    "WITH genai.vector.encode($question, \"OpenAI\", {...}) AS question_embedding\n",
    "  -- Convert question to embedding vector\n",
    "\n",
    "CALL db.index.vector.queryNodes($index_name, $top_k, question_embedding)\n",
    "  -- Search the vector index\n",
    "  YIELD node, score\n",
    "  \n",
    "RETURN score, node.text AS text\n",
    "  -- Return similarity score and text\n",
    "```\n",
    "\n",
    "### Understanding Similarity Scores\n",
    "\n",
    "Cosine similarity ranges from 0 to 1:\n",
    "- **0.95 - 1.0**: Extremely similar (likely exact match or paraphrase)\n",
    "- **0.85 - 0.95**: Very relevant\n",
    "- **0.75 - 0.85**: Moderately relevant\n",
    "- **< 0.75**: Weak match (may not be useful)\n",
    "\n",
    "### Example Query Walkthrough\n",
    "\n",
    "**Question**: \"In a single sentence, tell me about NetApp\"\n",
    "\n",
    "**What happens**:\n",
    "1. Question encoded: `[0.002, -0.015, 0.031, ..., 0.008]` (1536 dimensions)\n",
    "2. Index searched: Compares to all 257 chunk embeddings\n",
    "3. Top result: Item 1 Business Overview (score: ~0.92)\n",
    "4. Why? Contains \"NetApp, Inc.... is a global cloud-led, data-centric software company\"\n",
    "\n",
    "**Key insight**: We never searched for exact text matches! The system understood the semantic meaning of \"tell me about NetApp\" and found the business overview section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7047e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neo4j_vector_search(question):\n",
    "    \"\"\"Search for similar nodes using the Neo4j ector index\"\"\"\n",
    "    vector_search_query = \"\"\"\n",
    "        with genai.vector.encode(\n",
    "            $question,\n",
    "            \"OpenAI\",\n",
    "            {\n",
    "                token: $openAiApiKey,\n",
    "                endpoint: $openAiEndpoint\n",
    "            }) as question_embedding\n",
    "        call db.index.vector.queryNodes($index_name, $top_k, question_embedding) yield node, score\n",
    "        return score, node.text as text\n",
    "    \"\"\"\n",
    "    similar = kg.query(vector_search_query, \n",
    "                       params={\n",
    "                            'question': question, \n",
    "                            'openAiApiKey':OPENAI_API_KEY,\n",
    "                            'openAiEndpoint': OPENAI_ENDPOINT,\n",
    "                            'index_name':VECTOR_INDEX_NAME, \n",
    "                            'top_k': 10\n",
    "                       })\n",
    "    return similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "abb817f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>Item 1.  \\nBusiness\\n\\n\\nOverview\\n\\n\\nNetApp, Inc. (NetApp, we, us or the Company) is a global cloud-led, data-centric software company. We were incorporated in 1992 and are headquartered in San Jose, California. Building on more than three decades of innovation, we give customers the freedom to manage applications and data across hybrid multicloud environments. Our portfolio of cloud services, and storage infrastructure, powered by intelligent data management software, enables applications to run faster, more reliably, and more securely, all at a lower cost.\\n\\n\\nOur opportunity is defined by the durable megatrends of data-driven digital and cloud transformations. NetApp helps organizations meet the complexities created by rapid data and cloud growth, multi-cloud management, and the adoption of next-generation technologies, such as AI, Kubernetes, and modern databases. Our modern approach to hybrid, multicloud infrastructure and data management, which we term ‘evolved cloud’, provides customers the ability to leverage data across their entire estate with simplicity, security, and sustainability which increases our relevance and value to our customers.\\n\\n\\nIn an evolved cloud state, the cloud is fully integrated into an organization’s architecture and operations. Data centers and clouds are seamlessly united and hybrid multicloud operations are simplified, with consistency and observability across environments. The key benefits NetApp brings to an organization’s hybrid multicloud environment are:\\n\\n\\n•\\nOperational simplicity: NetApp’s use of open source, open architectures and APIs, microservices, and common capabilities and data services facilitate the creation of applications that can run anywhere.\\n\\n\\n•\\nFlexibility and consistency: NetApp makes moving data and applications between environments seamless through a common storage foundation across on-premises and multicloud environments.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results = neo4j_vector_search('In a single sentence, tell me about NetApp')\n",
    "search_results[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cecec2",
   "metadata": {},
   "source": [
    "## Step 8: Build a Complete RAG System with LangChain\n",
    "\n",
    "Now we'll integrate everything into an end-to-end question-answering system!\n",
    "\n",
    "### The RAG Pipeline Architecture\n",
    "\n",
    "```\n",
    "User Question\n",
    "    ↓\n",
    "[1] Encode question (OpenAI Embeddings)\n",
    "    ↓\n",
    "[2] Vector search in Neo4j (retrieve top-k chunks)\n",
    "    ↓\n",
    "[3] Pass chunks as context to LLM\n",
    "    ↓\n",
    "[4] Generate answer (ChatGPT)\n",
    "    ↓\n",
    "Return answer to user\n",
    "```\n",
    "\n",
    "### LangChain Components\n",
    "\n",
    "**Neo4jVector**: Wrapper for Neo4j vector store\n",
    "- Handles embedding generation\n",
    "- Performs vector searches\n",
    "- Returns results as Documents\n",
    "\n",
    "**Retriever**: Converts vector store into retriever interface\n",
    "- Standard LangChain interface\n",
    "- Can be chained with other components\n",
    "\n",
    "**RetrievalQAWithSourcesChain**: Complete RAG chain\n",
    "- Takes a question\n",
    "- Retrieves relevant chunks (via retriever)\n",
    "- Constructs prompt with context\n",
    "- Calls LLM (ChatGPT)\n",
    "- Returns answer + sources\n",
    "\n",
    "### Why \"WithSources\"?\n",
    "\n",
    "The `RetrievalQAWithSourcesChain` provides:\n",
    "1. **Answer**: The generated response\n",
    "2. **Sources**: Which chunks were used (provenance)\n",
    "\n",
    "**Critical for compliance and trust** in financial/legal domains!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f338104",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_vector_store = Neo4jVector.from_existing_graph(\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name=VECTOR_INDEX_NAME,\n",
    "    node_label=VECTOR_NODE_LABEL,\n",
    "    text_node_properties=[VECTOR_SOURCE_PROPERTY],\n",
    "    embedding_node_property=VECTOR_EMBEDDING_PROPERTY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8fbbf1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = neo4j_vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf788c",
   "metadata": {},
   "source": [
    "## Step 9: Test the RAG System\n",
    "\n",
    "Let's ask questions and evaluate the quality of responses!\n",
    "\n",
    "### Testing Strategy\n",
    "\n",
    "We'll test three scenarios:\n",
    "1. **In-scope questions**: About NetApp (should answer correctly)\n",
    "2. **Out-of-scope questions**: About other companies (tests hallucination control)\n",
    "3. **Prompt engineering**: How instructions affect behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1b69a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    ChatOpenAI(temperature=0), \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "31d75d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettychain(question: str) -> str:\n",
    "    \"\"\"Pretty print the chain's response to a question\"\"\"\n",
    "    response = chain({\"question\": question},\n",
    "        return_only_outputs=True,)\n",
    "    print(textwrap.fill(response['answer'], 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "46341586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetApp's primary business is enterprise storage and data\n",
      "management, cloud storage, and cloud operations.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Netapp's primary business?\"\n",
    "prettychain(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2unay0j8zw4",
   "metadata": {},
   "source": [
    "### Test 1: In-Scope Question\n",
    "\n",
    "**Question**: \"What is Netapp's primary business?\"\n",
    "\n",
    "**Expected**: Should retrieve business overview chunk and answer correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "925afa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Netapp is headquartered in San Jose, California.\n"
     ]
    }
   ],
   "source": [
    "prettychain(\"Where is Netapp headquartered?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "in118g8hdz",
   "metadata": {},
   "source": [
    "**Result**: Correct! Answered \"San Jose, California\" from the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1391de97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetApp is a global cloud-led, data-centric software company\n",
      "that empowers customers with hybrid multicloud solutions\n",
      "built for a better future.\n"
     ]
    }
   ],
   "source": [
    "prettychain(\"\"\"\n",
    "    Tell me about Netapp. \n",
    "    Limit your answer to a single sentence.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "482d2c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetApp is a global cloud-led, data-centric software company\n",
      "that provides organizations the ability to manage and share\n",
      "their data across on-premises, private, and public clouds.\n"
     ]
    }
   ],
   "source": [
    "prettychain(\"\"\"\n",
    "    Tell me about Apple. \n",
    "    Limit your answer to a single sentence.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t0mo84868p",
   "metadata": {},
   "source": [
    "### Test 2: Out-of-Scope Question (Hallucination Test)\n",
    "\n",
    "**Question**: \"Tell me about Apple.\"\n",
    "\n",
    "**Challenge**: Our database only contains NetApp documents. A good RAG system should either:\n",
    "1. Say \"I don't know\"\n",
    "2. Refuse to answer out-of-scope questions\n",
    "\n",
    "**What happened?** The system hallucinated! It returned information about NetApp, substituting it for Apple. This is a common RAG failure mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "438c573f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "prettychain(\"\"\"\n",
    "    Tell me about Apple. \n",
    "    Limit your answer to a single sentence.\n",
    "    If you are unsure about the answer, say you don't know.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6uj3nklm72v",
   "metadata": {},
   "source": [
    "### Test 3: Prompt Engineering to Control Hallucinations\n",
    "\n",
    "**Updated question**: \"Tell me about Apple. If you are unsure about the answer, say you don't know.\"\n",
    "\n",
    "**Result**: Success! The system now responds \"I don't know.\"\n",
    "\n",
    "### Key Lesson: Prompt Engineering Matters!\n",
    "\n",
    "**The fix**: Adding explicit instructions to the prompt:\n",
    "- \"If you are unsure, say you don't know\"\n",
    "- \"Only answer based on the provided context\"\n",
    "- \"Do not use external knowledge\"\n",
    "\n",
    "**Best practices for production RAG systems**:\n",
    "\n",
    "1. **System prompt engineering**:\n",
    "```python\n",
    "system_prompt = \"\"\"\n",
    "You are a financial document assistant. \n",
    "Only answer questions based on the provided SEC filing context.\n",
    "If the context doesn't contain the answer, respond: \"I don't have information about that in this document.\"\n",
    "Never use your general knowledge about companies.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "2. **Similarity threshold**: Only use chunks above a certain score (e.g., > 0.80)\n",
    "\n",
    "3. **Confidence scoring**: Return confidence levels with answers\n",
    "\n",
    "4. **Source attribution**: Always show which chunks were used\n",
    "\n",
    "**Why this matters in production**:\n",
    "- Legal liability (giving wrong financial advice)\n",
    "- Regulatory compliance (SEC, FINRA requirements)\n",
    "- User trust (transparency about limitations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drntp93da2c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary: Building Production-Ready Graph RAG Systems\n",
    "\n",
    "## What We Built\n",
    "\n",
    "A complete **Graph RAG pipeline** that:\n",
    "1. Ingests SEC 10-K filings (unstructured text)\n",
    "2. Chunks documents with overlap for context preservation\n",
    "3. Creates graph nodes with rich metadata\n",
    "4. Generates embeddings using OpenAI\n",
    "5. Indexes vectors for similarity search\n",
    "6. Builds a Q&A system with LangChain\n",
    "7. Controls hallucinations through prompt engineering\n",
    "\n",
    "## The Graph RAG Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                    User Question                        │\n",
    "└──────────────────────┬──────────────────────────────────┘\n",
    "                       │\n",
    "                       ↓\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│         1. Embed Question (OpenAI ada-002)              │\n",
    "└──────────────────────┬──────────────────────────────────┘\n",
    "                       │\n",
    "                       ↓\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│    2. Vector Search in Neo4j (Cosine Similarity)        │\n",
    "│       - Search 257 chunk embeddings                     │\n",
    "│       - Return top-10 most similar                      │\n",
    "└──────────────────────┬──────────────────────────────────┘\n",
    "                       │\n",
    "                       ↓\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│    3. Graph Traversal (Future Enhancement)              │\n",
    "│       - Filter by metadata (form section, company)      │\n",
    "│       - Follow relationships (NEXT_CHUNK, etc.)         │\n",
    "└──────────────────────┬──────────────────────────────────┘\n",
    "                       │\n",
    "                       ↓\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│    4. LLM Generation (ChatGPT with Context)             │\n",
    "│       - Pass chunks as context                          │\n",
    "│       - Generate answer                                 │\n",
    "│       - Return sources                                  │\n",
    "└──────────────────────┬──────────────────────────────────┘\n",
    "                       │\n",
    "                       ↓\n",
    "                   Answer\n",
    "```\n",
    "\n",
    "## Key Concepts Covered\n",
    "\n",
    "### 1. Document Chunking\n",
    "- **Challenge**: Balance context vs. embedding limits\n",
    "- **Solution**: 2000 chars with 200 char overlap\n",
    "- **Trade-off**: Smaller chunks = more precise, larger = more context\n",
    "\n",
    "### 2. Graph Node Design\n",
    "- **Pattern**: Use MERGE for idempotent operations\n",
    "- **Constraints**: Enforce uniqueness on chunkId\n",
    "- **Metadata**: Store CIK, CUSIP, section, source for filtering\n",
    "\n",
    "### 3. Vector Embeddings\n",
    "- **Model**: OpenAI text-embedding-ada-002 (1536 dimensions)\n",
    "- **Storage**: In-graph (single source of truth)\n",
    "- **Index**: Cosine similarity for text\n",
    "\n",
    "### 4. Hallucination Control\n",
    "- **Problem**: LLMs answer questions outside their context\n",
    "- **Solution**: Explicit prompt instructions (\"say you don't know\")\n",
    "- **Production**: Add similarity thresholds, confidence scores\n",
    "\n",
    "## Current Limitations & Future Enhancements\n",
    "\n",
    "### Limitations:\n",
    "1. **No entity extraction**: We only have text chunks, not structured entities\n",
    "2. **No relationships**: Chunks are isolated (no NEXT_CHUNK, REFERENCES, etc.)\n",
    "3. **Single document**: Only one company's 10-K form\n",
    "4. **Basic retrieval**: Pure vector search without graph traversal\n",
    "\n",
    "### Next Steps (Graph RAG Part 2):\n",
    "\n",
    "#### 1. Entity Extraction\n",
    "Extract entities from text and create dedicated nodes:\n",
    "```cypher\n",
    "(Company {name: \"NetApp\", cik: \"1002047\"})\n",
    "(Person {name: \"CEO Name\"})\n",
    "(Product {name: \"Cloud Storage\"})\n",
    "(Risk {category: \"Cybersecurity\"})\n",
    "```\n",
    "\n",
    "#### 2. Relationship Modeling\n",
    "Connect entities with meaningful relationships:\n",
    "```cypher\n",
    "(Company)-[:DISCLOSED_RISK]->(Risk)\n",
    "(Company)-[:OFFERS_PRODUCT]->(Product)\n",
    "(Person)-[:LEADS]->(Company)\n",
    "(Chunk)-[:MENTIONS]->(Company)\n",
    "(Chunk)-[:NEXT_CHUNK]->(Chunk)\n",
    "```\n",
    "\n",
    "#### 3. Multi-Document Knowledge Graph\n",
    "Load multiple companies' 10-K forms:\n",
    "```cypher\n",
    "(NetApp)-[:COMPETITOR_OF]->(Dell)\n",
    "(NetApp)-[:PARTNER_WITH]->(AWS)\n",
    "(NetApp)-[:INVESTED_IN]->(Startup)\n",
    "```\n",
    "\n",
    "#### 4. Advanced Queries\n",
    "Combine vector search + graph traversal:\n",
    "```cypher\n",
    "// Find competitors of NetApp who disclosed AI risks\n",
    "MATCH (netapp:Company {name: \"NetApp\"})\n",
    "MATCH (netapp)-[:COMPETITOR_OF]->(competitor)\n",
    "MATCH (competitor)-[:DISCLOSED_RISK]->(risk:Risk)\n",
    "WHERE risk.category CONTAINS \"AI\"\n",
    "MATCH (chunk:Chunk)-[:MENTIONS]->(risk)\n",
    "// Then do vector search on those chunks\n",
    "CALL db.index.vector.queryNodes(...)\n",
    "```\n",
    "\n",
    "#### 5. Temporal Analysis\n",
    "Track changes over time:\n",
    "```cypher\n",
    "(Company)-[:FILED {year: 2023}]->(Form10K)\n",
    "(Company)-[:FILED {year: 2024}]->(Form10K)\n",
    "\n",
    "// Compare risk disclosures year-over-year\n",
    "MATCH (c:Company)-[f1:FILED {year: 2023}]->(form1)\n",
    "MATCH (c)-[f2:FILED {year: 2024}]->(form2)\n",
    "MATCH (form1)<-[:PART_OF]-(chunk1:Chunk {section: \"risks\"})\n",
    "MATCH (form2)<-[:PART_OF]-(chunk2:Chunk {section: \"risks\"})\n",
    "// Vector search to find new risks in 2024\n",
    "```\n",
    "\n",
    "## Real-World Applications\n",
    "\n",
    "### 1. Investment Research\n",
    "**Query**: \"Which companies in the cloud storage sector have disclosed supply chain risks?\"\n",
    "- Vector search for \"supply chain risks\"\n",
    "- Graph traversal to find companies in \"cloud storage\" sector\n",
    "- Filter by disclosure year\n",
    "\n",
    "### 2. Compliance Monitoring\n",
    "**Query**: \"Has NetApp's cybersecurity risk disclosure changed since last year?\"\n",
    "- Fetch 2023 and 2024 risk sections\n",
    "- Vector similarity between years\n",
    "- Flag significant changes\n",
    "\n",
    "### 3. Competitive Intelligence\n",
    "**Query**: \"What new products has NetApp mentioned that Dell hasn't?\"\n",
    "- Extract product entities from both companies\n",
    "- Compare using set difference\n",
    "- Return chunks mentioning unique products\n",
    "\n",
    "### 4. Due Diligence\n",
    "**Query**: \"Show all companies connected to NetApp with investments over $50M and disclosed risks\"\n",
    "- Graph traversal: (NetApp)-[:INVESTED_IN]->(Portfolio Company)\n",
    "- Filter: investment amount > $50M\n",
    "- Vector search in portfolio company filings for risks\n",
    "\n",
    "## Production Checklist\n",
    "\n",
    "Before deploying a Graph RAG system:\n",
    "\n",
    "- [ ] **Data quality**: Clean, validated SEC filings\n",
    "- [ ] **Chunking strategy**: Tested for your document types\n",
    "- [ ] **Embedding model**: Chosen based on cost/performance\n",
    "- [ ] **Vector index tuning**: Right similarity function and dimensions\n",
    "- [ ] **Prompt engineering**: System prompts prevent hallucinations\n",
    "- [ ] **Similarity thresholds**: Filter low-relevance results\n",
    "- [ ] **Source attribution**: Always return provenance\n",
    "- [ ] **Error handling**: Graceful failures for API errors\n",
    "- [ ] **Monitoring**: Track query latency, relevance scores\n",
    "- [ ] **Compliance**: Legal review for financial advice disclaimers\n",
    "- [ ] **Security**: API key management, access controls\n",
    "- [ ] **Scalability**: Test with full dataset (1000+ forms)\n",
    "\n",
    "## Cost Analysis\n",
    "\n",
    "For 1000 SEC 10-K forms:\n",
    "\n",
    "| Component | Volume | Cost |\n",
    "|-----------|--------|------|\n",
    "| **Embedding generation** | ~100M chars | ~$10 (one-time) |\n",
    "| **Vector storage** | 1536-dim × 300K chunks | Neo4j license |\n",
    "| **Query embeddings** | 10K queries/month | ~$1/month |\n",
    "| **LLM generation** | 10K queries × 1K tokens | ~$200/month |\n",
    "\n",
    "**Total**: ~$10 setup + ~$200/month for moderate usage\n",
    "\n",
    "## Resources & Further Reading\n",
    "\n",
    "### Neo4j & Graph Databases\n",
    "- [Neo4j Cypher Manual](https://neo4j.com/docs/cypher-manual/)\n",
    "- [Vector Search in Neo4j](https://neo4j.com/docs/cypher-manual/current/indexes-for-vector-search/)\n",
    "- [GenAI Plugin Documentation](https://neo4j.com/docs/genai-plugin/)\n",
    "\n",
    "### LangChain & RAG\n",
    "- [LangChain Neo4j Integration](https://python.langchain.com/docs/integrations/vectorstores/neo4jvector/)\n",
    "- [RAG Best Practices](https://www.anthropic.com/index/retrieval-augmented-generation)\n",
    "\n",
    "### SEC Filings & Financial Data\n",
    "- [SEC EDGAR Database](https://www.sec.gov/edgar/search/)\n",
    "- [Understanding 10-K Forms](https://www.sec.gov/files/reada10k.pdf)\n",
    "- [XBRL for Structured Data](https://www.sec.gov/structureddata/osd-inline-xbrl.html)\n",
    "\n",
    "### Graph RAG Research\n",
    "- [Microsoft GraphRAG](https://github.com/microsoft/graphrag)\n",
    "- [Knowledge Graphs for LLMs (Stanford)](https://arxiv.org/abs/2306.04136)\n",
    "\n",
    "---\n",
    "\n",
    "## Next Tutorial: Entity Extraction & Multi-Hop Queries\n",
    "\n",
    "In the next part of this series, we'll:\n",
    "1. Use LLMs to extract entities (companies, people, products, risks)\n",
    "2. Build a multi-document knowledge graph\n",
    "3. Implement hybrid queries (vector search + graph traversal)\n",
    "4. Answer complex questions like: \"Which firms connected to Palo Alto Networks have invested over $50M and disclosed cybersecurity risks?\"\n",
    "\n",
    "**Stay tuned!**\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've built a complete Graph RAG system from scratch. You now understand:\n",
    "- Document chunking strategies\n",
    "- Graph node modeling with Cypher\n",
    "- Vector embeddings and similarity search\n",
    "- Retrieval augmented generation with LangChain\n",
    "- Hallucination control through prompt engineering\n",
    "\n",
    "This foundation prepares you for building advanced Graph RAG applications in finance, legal, healthcare, and beyond!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
